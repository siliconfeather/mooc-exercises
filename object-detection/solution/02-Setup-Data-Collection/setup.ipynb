{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "![](../images/dtlogo.png)\n",
    "\n",
    "\n",
    "\n",
    "# Object Detection\n",
    "\n",
    "Machine-learned object detection models can be extremely useful. They are faster and often more reliable than traditional computer vision models. Additionally, we can use pretrained model weights to cut down immensely on training time.\n",
    "\n",
    "Here's an example of what an object detector might output:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('3jD02dxL6gg', width=800, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, you will create your own Duckietown object detection dataset. You will learn about the general structure such a dataset should follow. You will train the object detection model on that dataset ([in a subsequent notebook](../03-Training/training.ipynb). Finally, you will integrate the model into a ROS node and test the integration ([in the last notebook](../04-Integration/integration.ipynb), so that your Duckiebot knows how to recognize duckie pedestrians (and thus avoid them). You can test your object detector in simulation and on your real Duckiebot.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Setup  \n",
    "2. Investigation\n",
    "3. Data collection\n",
    "4. Training\n",
    "5. Integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "### 1. Setup\n",
    "\n",
    "First, we need some global variables. These allow you to change the directory where we save of of the data you will need. You can also change the image size to reflect what your final model uses, but you can worry about that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET_DIR=\"/jupyter_ws/solution/duckietown_dataset\"\n",
    "IMAGE_SIZE = 416\n",
    "# this is the percentage of data that will go into the training set (as opposed to the testing set)\n",
    "SPLIT_PERCENTAGE = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "While you will build your own dataset with simulated images in part 2, it would be unreasonable to ask you to build your own dataset of real images. Run the cell bellow to download a dataset of labelled real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import runp\n",
    "\n",
    "runp(f\"rm -rf {DATASET_DIR}\")\n",
    "runp(f\"mkdir {DATASET_DIR}\")\n",
    "runp(f\"mkdir {DATASET_DIR}/images\")\n",
    "runp(f\"mkdir {DATASET_DIR}/labels\")\n",
    "runp(f\"mkdir {DATASET_DIR}/train\")\n",
    "runp(f\"mkdir {DATASET_DIR}/val\")\n",
    "runp(f\"mkdir {DATASET_DIR}/train/images\")\n",
    "runp(f\"mkdir {DATASET_DIR}/train/labels\")\n",
    "runp(f\"mkdir {DATASET_DIR}/val/images\")\n",
    "runp(f\"mkdir {DATASET_DIR}/val/labels\")\n",
    "\n",
    "# use <!> to have a download indicator\n",
    "!wget -O duckietown_object_detection_dataset.zip https://www.dropbox.com/s/bpd535fzmj1pz5w/duckietown%20object%20detection%20dataset-20201129T162330Z-001.zip?dl=0\n",
    "runp(f\"unzip -q duckietown_object_detection_dataset.zip -d {DATASET_DIR}\")\n",
    "runp(f\"mv {DATASET_DIR}/duckietown\\ object\\ detection\\ dataset/* {DATASET_DIR} && rm -rf {DATASET_DIR}/duckietown\\ object\\ detection\\ dataset\")\n",
    "runp(f\"rm duckietown_object_detection_dataset.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These real images are not the right size. Run the cell bellow to resize them (and resize the associated bounding boxes accordingly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import xminyminxmaxymax2xywfnormalized, train_test_split, makedirs, runp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(f\"{DATASET_DIR}/annotation/final_anns.json\") as anns:\n",
    "    annotations = json.load(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_index = 0\n",
    "\n",
    "all_image_names = []\n",
    "    \n",
    "def save_img(img, boxes, classes):\n",
    "    global npz_index\n",
    "    cv2.imwrite(f\"{DATASET_DIR}/images/real_{npz_index}.jpg\", img)\n",
    "    with open(f\"{DATASET_DIR}/labels/real_{npz_index}.txt\", \"w\") as f:\n",
    "        for i in range(len(boxes)):\n",
    "            f.write(f\"{classes[i]} \"+\" \".join(map(str,boxes[i]))+\"\\n\")\n",
    "    npz_index += 1\n",
    "    all_image_names.append(f\"real_{npz_index}\")\n",
    "\n",
    "filenames = tqdm(os.listdir(f\"{DATASET_DIR}/frames\"))\n",
    "for filename in filenames:\n",
    "    img = cv2.imread(f\"{DATASET_DIR}/frames/{filename}\")\n",
    "\n",
    "    orig_y, orig_x = img.shape[0], img.shape[1]\n",
    "    scale_y, scale_x = IMAGE_SIZE/orig_y, IMAGE_SIZE/orig_x\n",
    "\n",
    "    img = cv2.resize(img, (IMAGE_SIZE,IMAGE_SIZE))\n",
    "\n",
    "    boxes = []\n",
    "    classes = []\n",
    "\n",
    "    if filename not in annotations:\n",
    "        continue\n",
    "\n",
    "    for detection in annotations[filename]:\n",
    "        box = detection[\"bbox\"]\n",
    "        label = detection[\"cat_name\"]\n",
    "\n",
    "        if label not in [\"duckie\", \"cone\"]:\n",
    "            continue\n",
    "\n",
    "        orig_x_min, orig_y_min, orig_w, orig_h = box\n",
    "\n",
    "        x_min = int(np.round(orig_x_min * scale_x))\n",
    "        y_min = int(np.round(orig_y_min * scale_y))\n",
    "        x_max = x_min + int(np.round(orig_w * scale_x))\n",
    "        y_max = y_min + int(np.round(orig_h * scale_y))\n",
    "\n",
    "        boxes.append([x_min, y_min, x_max, y_max])\n",
    "        classes.append(1 if label == \"duckie\" else 2)\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        continue\n",
    "\n",
    "\n",
    "    boxes = np.array([xminyminxmaxymax2xywfnormalized(box, IMAGE_SIZE) for box in boxes])\n",
    "    classes = np.array(classes)-1\n",
    "    \n",
    "    save_img(img, boxes, classes)\n",
    "\n",
    "\n",
    "\n",
    "train_test_split(all_image_names, SPLIT_PERCENTAGE, DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that's done, you're all set! We'll explain what all the code above was for later in this notebook.\n",
    "\n",
    "### Investigation\n",
    "\n",
    "What does an object detection dataset look like? Clearly, the specifics will depend on the convention used by specific models, but the general idea is intuitive:\n",
    "\n",
    "- We need an image\n",
    "- This image might have many bounding boxes in it, so we need some sort of list of coordinates\n",
    "- These bounding boxes must be associated with a class\n",
    "\n",
    "How are the bounding boxes defined?\n",
    "\n",
    "![image of a bounding box](../images/bbox.png)\n",
    "\n",
    "\\[Note: if you are not colorblind, you may ignore the scribbles under the colored indications for widths and heights\\]\n",
    "\n",
    "Some conventions use `x_min y_min width height`, whereas others use `x_min y_min x_max y_max`, and others use `x_center y_center width height`. In this exercise, the model we recommend ([YoloV5](https://github.com/Velythyl/yolov5)) uses `x_center y_center width height`.\n",
    "\n",
    "And how do we actually obtain these bounding boxes? In real-life applications, you would need to label a dataset of images by hand. But if you have access to a simulator that is able to segment images, you could obtain the bounding boxes directly from the segmented images. \n",
    "\n",
    "If you take a look at Pytorch's object detection [tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html), that is similar to what they do. While their images were segmented by hand, the tutorial uses the same technique that we will use here to obtain the bounding boxes. Their images look like this:\n",
    "\n",
    "![image with bounding boxes](../images/FudanPed.png)\n",
    "<p align=\"center\">\\[Source: https://www.cis.upenn.edu/~jshi/ped\\_html/\\]</p>\n",
    "\n",
    "And they simply calculate the min and max x and y coordinates of the segmented objects to obtain the bounding box.\n",
    "\n",
    "We will use the segmented mode in the Duckietown simulator to compute the bounding boxes of non-segmented images.\n",
    "\n",
    "#### What we want to detect\n",
    "\n",
    "The goal of this exercise is to make Duckietown safer: we want to be able to detect duckie pedestrians on the road and avoid squishing them. We also want to detect trucks, buses, and cones. Here is the complete list, along with their corresponding IDs:\n",
    "\n",
    "0. Duckie\n",
    "1. Cone\n",
    "2. Truck\n",
    "3. Bus\n",
    "\n",
    "\n",
    "## Data collection\n",
    "\n",
    "\n",
    "#### Format\n",
    "\n",
    "We are going to supplement the data from the real dataset that we already downloaded with data automatically generated from the simulator. \n",
    "\n",
    "The script we will use for this is the [../utils/data_collection.py](../utils/data_collection.py) file. You can edit it manually but the functions that are defined later in this notebook are automatically used by the script so unless you are interested to see all of the details, you can just continue with this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the script in the noVNC browser. **NB: if this doesn't work you should rerun `dts exercises lab` with the `--vnc` flag**. \n",
    "\n",
    "Navigate to [http://localhost:8087](http://localhost:8087) and click the \"Data Collection\" icon on the desktop. This will run your [../utils/data_collection.py](../utils/data_collection.py) script. If you edit the functions below, you simply need to rerun `dts exercises build` and then re-click on the data collection. The functions are pulled from this notebook and imported into the [../utils/data_collection.py](../utils/data_collection.py) script. \n",
    "\n",
    "The purpose of that [../utils/data_collection.py](../utils/data_collection.py) is to automatically generate data for you from the simulator. In the rest of this activity we will walk step by step through the process. \n",
    "\n",
    "Of course, your dataset's format depends heavily on your model. If you want to use the [YoloV5](https://github.com/duckietown/yolov5) model that we suggest, you should colosely follow their [guide on how to train using custom data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data).\n",
    "\n",
    "Your data should follow the following directory structure:\n",
    "\n",
    "![image of dataset save format](../images/dataset_format.png)\n",
    "\n",
    "The dataset is called `duckietown_dataset` and is located in the parent directory of this notebook. We have created two subdirectories in that folder: `train` and `val`. Inside `train` and `val`, there must again be two subdirectories `images` and `labels`. Inside `images`, you must place your images, and inside `labels`, you must place the images' bounding box data. Notice that the label files use the same name as their corresponding image files but with a different extension. In other words, the data for `0.jpg` can be found in `0.txt`.\n",
    "\n",
    "The format for the label files is fairly simple. For each bounding box in the corresponding image, write a row of the form `class x_center y_center width height`. Keep in mind that the pixel data must be 0-to-1 normalized (i.e., you can calculate the usual `x_center y_center width height` in pixel space and divide by the image's size). For example,\n",
    "\n",
    "    0 0.5 0.5 0.2 0.2\n",
    "    1 0.60 0.70 0.4 0.2\n",
    "\n",
    "this says \"there is a duckie (class 0) centered in the image, whose width and height are 20% of the image's. There is also a cone (class 1) whose center is at 60% of the image's maximal x value and 70% of the image's maximal y value, and its width is 40% of the image's own while its height is 20%.\"\n",
    "\n",
    "It is recommended that you read the guide posted on YoloV5's GitHub: [guide on how to train using custom data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating data\n",
    "\n",
    "1. Take the segmented image (this is provided to you by the simulator's rendering engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "mapping = {\n",
    "    \"house\": \"3deb34\",\n",
    "    \"bus\": \"ebd334\",\n",
    "    \"truck\": \"961fad\",\n",
    "    \"duckie\": \"cfa923\",\n",
    "    \"cone\": \"ffa600\",\n",
    "    \"floor\": \"000000\",\n",
    "    \"grass\": \"000000\",\n",
    "    \"barrier\": \"000099\"\n",
    "}\n",
    "mapping = {\n",
    "    key:\n",
    "        [int(h[i:i+2], 16) for i in (0,2,4)]\n",
    "    for key, h in mapping.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feel free to experiment with a few other files in the images folder. All of the original/segmented pairs are labeled as *_not_seg and *_seg\n",
    "obs = np.asarray(Image.open('../images/duckie_not_seg.png'))\n",
    "obs_seg = np.asarray(Image.open('../images/duckie_seg.png'))\n",
    "# define the mapping from objects to colours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For each color in the interesting colors (so the colors for duckies, trucks, busses, and cones):\n",
    "   \n",
    "   1. Remove all other colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "def segmented_image_one_class(segmented_img, class_name):\n",
    "    mask = np.all(segmented_img == mapping[class_name], axis=-1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckie_masked_image = segmented_image_one_class(np.asarray(obs_seg),\"duckie\")\n",
    "plt.imshow(duckie_masked_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    B. Then take that image and use it to find bounding boxes around each unique instance within the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "def find_all_bboxes(mask):\n",
    "    gray = mask.astype(\"uint8\")\n",
    "    gray[mask == True] = 255\n",
    "    gray[mask == False] = 0\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "\n",
    "    boxes = []\n",
    "    for index, cnt in enumerate(contours):\n",
    "        if hierarchy[0,index,3] != -1:\n",
    "            continue\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        boxes.append([x,y,w+x,h+y])\n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_image_with_boxes(img, boxes):\n",
    "    import matplotlib.patches as patches\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    for box in boxes:\n",
    "        rect = patches.Rectangle((box[0], box[2]), box[1]-box[0], box[3]-box[2], linewidth=1, edgecolor='w', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = find_all_bboxes(duckie_masked_image)\n",
    "show_image_with_boxes(obs,boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    C. Now we want a function that does the above but for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "def find_all_boxes_and_classes(segmented_img):\n",
    "\n",
    "    classes = [\"duckie\", \"cone\", \"truck\", \"bus\"]\n",
    "    all_boxes = []\n",
    "    all_classes = []\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        mask = segmented_image_one_class(segmented_img, class_name)\n",
    "        boxes = find_all_bboxes(mask)\n",
    "        all_boxes.extend(list(boxes))\n",
    "        classes = np.array([i]*boxes.shape[0])\n",
    "        all_classes.extend(list(classes))\n",
    "\n",
    "    return all_boxes, all_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_boxes, all_classes = find_all_boxes_and_classes(obs_seg)\n",
    "show_image_with_boxes(obs, all_boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will need to save the non-segmented version of the image, and write its bounding boxes + their classes to a corresponding txt file. You can see how this is done in [../utils/data_collection.py](../utils/data_collection.py)\n",
    "\n",
    "\n",
    "#### Combining with the real dataset & training/test set splits\n",
    "\n",
    "When training supervised learning models, one must worry about overfitting to your training set. You should always keep *some* of your dataset *out* of your training data. This way, you can verify that your model does not overfit to your dataset by *testing* it on the data you left out. \n",
    "\n",
    "You can experiment with the `SPLIT_PERCENTAGE` variable defined at the top of this notebook to adjust the percentage of the **real** data that is used for training as opposed to testing. There is a similar variable defined in [../utils/data_collection.py](../data_collection.py) which controls the percentage of the **simulated** data that will be used for training. \n",
    "\n",
    "## Finishing up\n",
    "\n",
    "Make sure if you modified these functions or the code in [../utils/data_collection.py](../data_collection.py) that you **rerun `dts exercises build`** and go back to the [novnc browser](http://localhost:8087) and re-run the data generation procedure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step\n",
    "\n",
    "You can continue with the [Training notebook](../03-Training/training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
